{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62c39936-f17d-4215-b8fb-5a6763586783",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b64ef226-2f3c-4646-892d-630ebc3cc4e6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "TRANSACTION_TABLE = 'tran'\n",
    "TRANSACTION_ID_COLUMN = 'tid'\n",
    "\n",
    "LHS_COLUMNS_LIST = ['diagcode']\n",
    "#LHS_COLUMNS_LIST = ['diagcode', 'servprov']\n",
    "RHS_COLUMN = 'servcode'\n",
    "\n",
    "LHS_TABLES = ['diag']\n",
    "LHS_TABLES_JOIN_COLUMNS = ['dimDiagnosisID']\n",
    "LHS_TABLES_DESC_COLUMNS = ['DiagnosisShortDesc']\n",
    "\n",
    "# LHS_TABLES = ['diag', 'provcleaned']\n",
    "# LHS_TABLES_JOIN_COLUMNS = ['dimDiagnosisID', 'dimProviderID']\n",
    "# LHS_TABLES_DESC_COLUMNS = ['DiagnosisShortDesc', 'ProviderName']\n",
    "\n",
    "RHS_TABLE = 'serv'\n",
    "RHS_TABLE_JOIN_COLUMN = 'dimServiceCodeID'\n",
    "RHS_TABLE_DESC_COLUMN = 'ServiceCodeShortDesc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a292aa82-142d-4af8-be40-728bec34e027",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "transactions_df = sqlContext.sql(f'select * from {TRANSACTION_TABLE}')\n",
    "transactions_df = transactions_df.distinct()\n",
    "transactions_df.createOrReplaceTempView('transactions_df_sql')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7143fdc-ad3b-48c3-8fe6-c38f38f002ee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tid_count_total = spark.sql(f'\\\n",
    "                            select count(distinct {TRANSACTION_ID_COLUMN}) as tid_count_total \\\n",
    "                            from transactions_df_sql \\\n",
    "                        ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3b93c2d-128f-43b2-9ac0-7e8e6912e532",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if len(LHS_COLUMNS_LIST) == 1:\n",
    "    tid_count_lhs = spark.sql(f'\\\n",
    "                                select {LHS_COLUMNS_LIST[0]}, count(distinct {TRANSACTION_ID_COLUMN}) as tid_count_lhs \\\n",
    "                                from transactions_df_sql \\\n",
    "                                group by {LHS_COLUMNS_LIST[0]} \\\n",
    "                            ')\n",
    "elif len(LHS_COLUMNS_LIST) == 2:\n",
    "    tid_count_lhs = spark.sql(f'\\\n",
    "                                select {LHS_COLUMNS_LIST[0]}, {LHS_COLUMNS_LIST[1]}, count(distinct {TRANSACTION_ID_COLUMN}) as tid_count_lhs \\\n",
    "                                from transactions_df_sql \\\n",
    "                                group by {LHS_COLUMNS_LIST[0]}, {LHS_COLUMNS_LIST[1]} \\\n",
    "                            ')\n",
    "elif len(LHS_COLUMNS_LIST) > 2:\n",
    "    print('Only two columns allowed at a time in LHS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c101b82c-b675-4ade-ba65-e12d7b0c9b32",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tid_count_rhs = spark.sql(f'\\\n",
    "                            select {RHS_COLUMN}, count(distinct {TRANSACTION_ID_COLUMN}) as tid_count_rhs \\\n",
    "                            from transactions_df_sql \\\n",
    "                            group by {RHS_COLUMN} \\\n",
    "                        ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55652076-81ac-4cab-9361-c4be11edc422",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if len(LHS_COLUMNS_LIST) == 1:\n",
    "    tid_count_lhs_rhs = spark.sql(f'\\\n",
    "                                    select {LHS_COLUMNS_LIST[0]}, {RHS_COLUMN}, count(distinct {TRANSACTION_ID_COLUMN}) as tid_count_lhs_rhs \\\n",
    "                                    from transactions_df_sql \\\n",
    "                                    group by {LHS_COLUMNS_LIST[0]}, {RHS_COLUMN} \\\n",
    "                                ')\n",
    "elif len(LHS_COLUMNS_LIST) == 2:\n",
    "    tid_count_lhs_rhs = spark.sql(f'\\\n",
    "                                    select {LHS_COLUMNS_LIST[0]}, {LHS_COLUMNS_LIST[1]}, {RHS_COLUMN}, count(distinct {TRANSACTION_ID_COLUMN}) as tid_count_lhs_rhs \\\n",
    "                                    from transactions_df_sql \\\n",
    "                                    group by {LHS_COLUMNS_LIST[0]}, {LHS_COLUMNS_LIST[1]}, {RHS_COLUMN} \\\n",
    "                                ')\n",
    "elif len(LHS_COLUMNS_LIST) > 2:\n",
    "    print('Only two columns allowed at a time in LHS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2db58f09-5313-4dc2-b4e9-7c35fabed5b0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tid_count_total.createOrReplaceTempView('tid_count_total_sql')\n",
    "tid_count_lhs.createOrReplaceTempView('tid_count_lhs_sql')\n",
    "tid_count_rhs.createOrReplaceTempView('tid_count_rhs_sql')\n",
    "tid_count_lhs_rhs.createOrReplaceTempView('tid_count_lhs_rhs_sql')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80c75db4-2e4b-4507-b949-511b5d25c73c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if len(LHS_COLUMNS_LIST) == 1:\n",
    "    lift_df = spark.sql(f'select l.{LHS_COLUMNS_LIST[0]}, r.{RHS_COLUMN}, \\\n",
    "                            lr.tid_count_lhs_rhs/t.tid_count_total as support, \\\n",
    "                            lr.tid_count_lhs_rhs/l.tid_count_lhs   as confidence, \\\n",
    "                            r.tid_count_rhs/t.tid_count_total as expected_confidence, \\\n",
    "                            (lr.tid_count_lhs_rhs*t.tid_count_total)/(l.tid_count_lhs*r.tid_count_rhs) as lift \\\n",
    "                            from tid_count_lhs_sql     as l \\\n",
    "                            join tid_count_rhs_sql     as r \\\n",
    "                            join tid_count_total_sql   as t \\\n",
    "                            join tid_count_lhs_rhs_sql as lr on (lr.{LHS_COLUMNS_LIST[0]}=l.{LHS_COLUMNS_LIST[0]}) \\\n",
    "                                                            and (lr.{RHS_COLUMN}=r.{RHS_COLUMN}) \\\n",
    "                        ')\n",
    "elif len(LHS_COLUMNS_LIST) == 2:\n",
    "    lift_df = spark.sql(f'select l.{LHS_COLUMNS_LIST[0]}, l.{LHS_COLUMNS_LIST[1]}, r.{RHS_COLUMN}, \\\n",
    "                            lr.tid_count_lhs_rhs/t.tid_count_total as support, \\\n",
    "                            lr.tid_count_lhs_rhs/l.tid_count_lhs   as confidence, \\\n",
    "                            r.tid_count_rhs/t.tid_count_total      as expected_confidence, \\\n",
    "                            (lr.tid_count_lhs_rhs*t.tid_count_total)/(l.tid_count_lhs*r.tid_count_rhs) as lift \\\n",
    "                            from tid_count_lhs_sql     as l \\\n",
    "                            join tid_count_rhs_sql     as r \\\n",
    "                            join tid_count_total_sql   as t \\\n",
    "                            join tid_count_lhs_rhs_sql as lr on (lr.{LHS_COLUMNS_LIST[0]}=l.{LHS_COLUMNS_LIST[0]}) \\\n",
    "                                                            and (lr.{LHS_COLUMNS_LIST[1]}=l.{LHS_COLUMNS_LIST[1]}) \\\n",
    "                                                            and (lr.{RHS_COLUMN}=r.{RHS_COLUMN}) \\\n",
    "                        ')\n",
    "elif len(LHS_COLUMNS_LIST) > 2:\n",
    "    print('Only two columns allowed at a time in LHS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "317a03e2-2913-4d99-9a77-4924f6c804d1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "lift_df.createOrReplaceTempView('lift_df_sql')\n",
    "RHS_df = sqlContext.sql(f'select * from {RHS_TABLE}')\n",
    "RHS_df.createOrReplaceTempView('RHS_df_sql')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e78252dc-8470-4aad-9015-f9a264c72c00",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------------------+--------------------+--------------------+------------------+\n|diagcode|servcode|             support|          confidence| expected_confidence|              lift|\n+--------+--------+--------------------+--------------------+--------------------+------------------+\n|   43753|    3923|3.274322760908952E-4|                 1.0|  0.7062277618912488|1.4159737891385653|\n|   32981|     715|2.182881840605968E-5|0.009615384615384616|6.548645521817905E-5| 146.8301282051282|\n|   36062|    1527|2.182881840605968E-5|                 0.2|8.731527362423872E-5|           2290.55|\n|   37369|    1210|1.746305472484774...| 0.19047619047619047|0.001287900285957...|147.89669087974173|\n+--------+--------+--------------------+--------------------+--------------------+------------------+\nonly showing top 4 rows\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select * from lift_df_sql').show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "200f38b4-fcce-492d-98f4-80563afef650",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if len(LHS_COLUMNS_LIST) == 1:\n",
    "    LHS_0_df = sqlContext.sql(f'select * from {LHS_TABLES[0]}')\n",
    "    LHS_0_df.createOrReplaceTempView('LHS_0_df_sql')\n",
    "    res_df = spark.sql(f'\\\n",
    "                            select lft.*, \\\n",
    "                                   lhs0df.{LHS_TABLES_DESC_COLUMNS[0]}, \\\n",
    "                                   rhsdf.{RHS_TABLE_DESC_COLUMN} \\\n",
    "                            from lift_df_sql  lft \\\n",
    "                            join LHS_0_df_sql lhs0df on lhs0df.{LHS_TABLES_JOIN_COLUMNS[0]}=lft.{LHS_COLUMNS_LIST[0]} \\\n",
    "                            join RHS_df_sql   rhsdf  on rhsdf.{RHS_TABLE_JOIN_COLUMN}=lft.{RHS_COLUMN} \\\n",
    "                    ')\n",
    "elif len(LHS_COLUMNS_LIST) == 2:\n",
    "    LHS_0_df = sqlContext.sql(f'select * from {LHS_TABLES[0]}')\n",
    "    LHS_0_df.createOrReplaceTempView('LHS_0_df_sql')\n",
    "    LHS_1_df = sqlContext.sql(f'select * from {LHS_TABLES[1]}')\n",
    "    LHS_1_df.createOrReplaceTempView('LHS_1_df_sql')\n",
    "    res_df = spark.sql(f'\\\n",
    "                            select lft.*, \\\n",
    "                                   lhs0df.{LHS_TABLES_DESC_COLUMNS[0]}, \\\n",
    "                                   lhs1df.{LHS_TABLES_DESC_COLUMNS[1]}, \\\n",
    "                                   rhsdf.{RHS_TABLE_DESC_COLUMN} \\\n",
    "                            from lift_df_sql  lft \\\n",
    "                            join LHS_0_df_sql lhs0df on lhs0df.{LHS_TABLES_JOIN_COLUMNS[0]}=lft.{LHS_COLUMNS_LIST[0]} \\\n",
    "                            join LHS_1_df_sql lhs1df on lhs1df.{LHS_TABLES_JOIN_COLUMNS[1]}=lft.{LHS_COLUMNS_LIST[1]} \\\n",
    "                            join RHS_df_sql   rhsdf  on rhsdf.{RHS_TABLE_JOIN_COLUMN}=lft.{RHS_COLUMN} \\\n",
    "                    ')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f1ada4d-cf8b-482f-8a80-d382a9155e70",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------------------+--------------------+--------------------+------------------+--------------------+--------------------+\n|diagcode|servcode|             support|          confidence| expected_confidence|              lift|  DiagnosisShortDesc|ServiceCodeShortDesc|\n+--------+--------+--------------------+--------------------+--------------------+------------------+--------------------+--------------------+\n|   43753|    3923|3.274322760908952E-4|                 1.0|  0.7062277618912488|1.4159737891385653|         PLEURODYNIA|ROUTINE VENIPUNCTURE|\n|   32981|     715|2.182881840605968E-5|0.009615384615384616|6.548645521817905E-5| 146.8301282051282|   ACTINIC KERATOSIS|SHAVE SKIN LESION...|\n|   36062|    1527|2.182881840605968E-5|                 0.2|8.731527362423872E-5|           2290.55|OTHER SPONDYLOSIS...|ARTHRD ANT INTERB...|\n|   37369|    1210|1.746305472484774...| 0.19047619047619047|0.001287900285957...|147.89669087974173|             MYALGIA|INJ TRIGGER POINT...|\n|   28471|    6738|5.675492785575517E-4|                 1.0|0.001833620746109...| 545.3690476190476|AGE-RELATED NUCLE...|XCAPSL CTRC RMVL ...|\n|   23223|    3923|7.858374626181485E-4|                 1.0|  0.7062277618912488|1.4159737891385653|UNS VIRAL HEPATIT...|ROUTINE VENIPUNCTURE|\n|   58657|     636|4.365763681211936E-5|                 1.0|8.731527362423872E-5|          11452.75|SUPERFICIAL FOREI...| REMOVE FOREIGN BODY|\n|   37400|    1208|2.182881840605968E-5| 0.06666666666666667|0.002335683569448386|28.542679127725858|   PAIN IN LEFT FOOT|INJ TENDON SHEATH...|\n|   33874|    3923|1.746305472484774...|                 0.8|  0.7062277618912488|1.1327790313108521|RA WITH RHEUMATOI...|ROUTINE VENIPUNCTURE|\n|   25314|    3924|4.365763681211936E-5| 0.00819672131147541|0.005500862238327039|1.4900793650793651|  ANEMIA UNSPECIFIED|CAPILLARY BLOOD DRAW|\n|   23080|    3923|4.365763681211936E-5|                 1.0|  0.7062277618912488|1.4159737891385653|VIRAL MENINGITIS ...|ROUTINE VENIPUNCTURE|\n|   58951|     813|8.731527362423872E-5|  0.5714285714285714|0.005893780969636114| 96.95449735449735|LAC W/O FB LT LIT...|RPR S/N/AX/GEN/TR...|\n|   24883|     927|2.182881840605968E-5|                 1.0|1.091440920302984E-4|            9162.2|BENIGN NEOPLASM M...|SKIN FULL GRAFT E...|\n|   31464|    3936|2.182881840605968E-5|                 1.0|4.147475497151339E-4|2411.1052631578946|VARICOSE VEINS RT...|ENDOVENOUS RF 1ST...|\n|   32433|   18664|2.182881840605968E-5|                 0.2|1.091440920302984E-4|           1832.44|   DYSPLASIA OF ANUS|DIAGNOSTIC ANOSCO...|\n|   56264|    2723|1.746305472484774...|                 0.5|0.001746305472484...|         286.31875|UNS FX LOWER LT R...|APPLY FOREARM SPLINT|\n|   32411|    5221|2.182881840605968E-5|                 1.0|1.309729104363581E-4| 7635.166666666667|OTHER SPECIFIED F...|REPLACE G-J TUBE ...|\n|   23643|    3050|4.365763681211936E-5|                 1.0|0.003798214402654...| 263.2816091954023|MALIGNANT NEOPLAS...|LARYNGOSCOPY FLEX...|\n|   36789|    1975|2.182881840605968E-5|                 0.2|5.020628233393727E-4| 398.3565217391304|TRIGGER FINGER RI...|INCISE FINGER TEN...|\n|   32770|     626|2.182881840605968E-5|                 1.0|0.004518565410054354|221.30917874396135|  CELLULITIS OF NECK|DRAINAGE OF SKIN ...|\n+--------+--------+--------------------+--------------------+--------------------+------------------+--------------------+--------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "res_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "beccf4fa-8443-45b1-8931-3ad3b68e4a67",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "res_df.write.option('header',True).csv('associations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ee3d341-051f-40f0-8fce-698d57944160",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "association_mining",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
